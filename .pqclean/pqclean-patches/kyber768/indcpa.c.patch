--- work/kyber768/scheme/indcpa.c
+++ work-patched/kyber768/scheme/indcpa.c
@@ -279,33 +279,33 @@
         buflen = off + XOF_BLOCKBYTES;
         ctr0 += rej_uniform(&(a[2][2][0]) + ctr0, KYBER_N - ctr0, buf0, buflen);
     }
+    shake128_ctx_release(&c_state);
 
     #elif KYBER_K == 4
     for (unsigned int i = 0; i < KYBER_K; i++) {
-        for (unsigned int j = 0; j < KYBER_K; j += 2) {
-            if (transposed) {
-                neon_xof_absorb(&state, seed, i, i, j, j + 1);
-            } else {
-                neon_xof_absorb(&state, seed, j, j + 1, i, i);
-            }
+        if (transposed) {
+            neon_xof_absorb(&state, seed, i, i, 0, 1);
+        } else {
+            neon_xof_absorb(&state, seed, 0, 1, i, i);
+        }
+
+        neon_xof_squeezeblocks(buf0, buf1, GEN_MATRIX_NBLOCKS, &state);
 
-            neon_xof_squeezeblocks(buf0, buf1, GEN_MATRIX_NBLOCKS, &state);
-            buflen = GEN_MATRIX_NBLOCKS * XOF_BLOCKBYTES;
-            ctr0 = neon_rej_uniform(&(a[i][j][0]), buf0);
-            ctr1 = neon_rej_uniform(&(a[i][j + 1][0]), buf1);
-
-            while (ctr0 < KYBER_N || ctr1 < KYBER_N) {
-                off = buflen % 3;
-                for (k = 0; k < off; k++) {
-                    buf0[k] = buf0[buflen - off + k];
-                    buf1[k] = buf1[buflen - off + k];
-                }
-                neon_xof_squeezeblocks(buf0 + off, buf1 + off, 1, &state);
-
-                buflen = off + XOF_BLOCKBYTES;
-                ctr0 += rej_uniform(&(a[i][j][0]) + ctr0, KYBER_N - ctr0, buf0, buflen);
-                ctr1 += rej_uniform(&(a[i][j + 1][0]) + ctr1, KYBER_N - ctr1, buf1, buflen);
+        buflen = GEN_MATRIX_NBLOCKS * XOF_BLOCKBYTES;
+
+        ctr0 = neon_rej_uniform(&(a[i][0][0]), buf0);
+        ctr1 = neon_rej_uniform(&(a[i][1][0]), buf1);
+        while (ctr0 < KYBER_N || ctr1 < KYBER_N) {
+            off = buflen % 3;
+            for (k = 0; k < off; k++) {
+                buf0[k] = buf0[buflen - off + k];
+                buf1[k] = buf1[buflen - off + k];
             }
+            neon_xof_squeezeblocks(buf0 + off, buf1 + off, 1, &state);
+
+            buflen = off + XOF_BLOCKBYTES;
+            ctr0 += rej_uniform(&(a[i][0][0]) + ctr0, KYBER_N - ctr0, buf0, buflen);
+            ctr1 += rej_uniform(&(a[i][1][0]) + ctr1, KYBER_N - ctr1, buf1, buflen);
         }
     }
     #else
@@ -359,11 +359,11 @@
     neon_polyvec_ntt(e);
 
     for (i = 0; i < KYBER_K; i++) {
-        __asm_point_mul_extended(&(skpv_asymmetric[i][0]), &(skpv[i][0]), pre_asymmetric_table_Q1_extended, asymmetric_const);
+        PQCLEAN_NAMESPACE__asm_point_mul_extended(&(skpv_asymmetric[i][0]), &(skpv[i][0]), pre_asymmetric_table_Q1_extended, asymmetric_const);
     }
 
     for (i = 0; i < KYBER_K; i++) {
-        __asm_asymmetric_mul_montgomery(&(a[i][0][0]), &(skpv[0][0]), &(skpv_asymmetric[0][0]), asymmetric_const, pkpv[i]);
+        PQCLEAN_NAMESPACE__asm_asymmetric_mul_montgomery(&(a[i][0][0]), &(skpv[0][0]), &(skpv_asymmetric[0][0]), asymmetric_const, pkpv[i]);
     }
 
     neon_polyvec_add_reduce(pkpv, e);
@@ -439,14 +439,14 @@
     neon_polyvec_ntt(sp);
 
     for (i = 0; i < KYBER_K; i++) {
-        __asm_point_mul_extended(&(sp_asymmetric[i][0]), &(sp[i][0]), pre_asymmetric_table_Q1_extended, asymmetric_const);
+        PQCLEAN_NAMESPACE__asm_point_mul_extended(&(sp_asymmetric[i][0]), &(sp[i][0]), pre_asymmetric_table_Q1_extended, asymmetric_const);
     }
 
     for (i = 0; i < KYBER_K; i++) {
-        __asm_asymmetric_mul(&(at[i][0][0]), &(sp[0][0]), &(sp_asymmetric[0][0]), asymmetric_const, b[i]);
+        PQCLEAN_NAMESPACE__asm_asymmetric_mul(&(at[i][0][0]), &(sp[0][0]), &(sp_asymmetric[0][0]), asymmetric_const, b[i]);
     }
 
-    __asm_asymmetric_mul(&(pkpv[0][0]), &(sp[0][0]), &(sp_asymmetric[0][0]), asymmetric_const, v);
+    PQCLEAN_NAMESPACE__asm_asymmetric_mul(&(pkpv[0][0]), &(sp[0][0]), &(sp_asymmetric[0][0]), asymmetric_const, v);
 
     neon_polyvec_invntt_to_mont(b);
     invntt(v);
@@ -487,10 +487,10 @@
     neon_polyvec_ntt(b);
 
     for (i = 0; i < KYBER_K; i++) {
-        __asm_point_mul_extended(&(b_asymmetric[i][0]), &(b[i][0]), pre_asymmetric_table_Q1_extended, asymmetric_const);
+        PQCLEAN_NAMESPACE__asm_point_mul_extended(&(b_asymmetric[i][0]), &(b[i][0]), pre_asymmetric_table_Q1_extended, asymmetric_const);
     }
 
-    __asm_asymmetric_mul(&(skpv[0][0]), &(b[0][0]), &(b_asymmetric[0][0]), asymmetric_const, mp);
+    PQCLEAN_NAMESPACE__asm_asymmetric_mul(&(skpv[0][0]), &(b[0][0]), &(b_asymmetric[0][0]), asymmetric_const, mp);
 
     invntt(mp);
 

